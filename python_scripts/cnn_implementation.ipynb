{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_947583/3907235755.py:16: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  from distutils.util import strtobool\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "from typing import List, Tuple\n",
    "from distutils.util import strtobool\n",
    "\n",
    "\n",
    "\n",
    "# seed the random number generators\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "def manual_seed(seed: int):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining class to obtain the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, dataset_path: PathLike, transform=None):\n",
    "\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        # determine the total dataset length\n",
    "        i = 0\n",
    "        while (self.dataset_path / f\"image{i}.npz\").exists():\n",
    "            i += 1\n",
    "        self.len = i\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        image = np.array(np.load(self.dataset_path / f\"image{idx}.npz\")['image'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            # print(\"Image shape after transformation:\", np.shape(x))  # Debugging\n",
    "\n",
    "        label = np.load(self.dataset_path / f\"label{idx}.npz\")['label']\n",
    "                \n",
    "        return image, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total number of samples available in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            self.len: The total number of images (or samples) in the dataset.\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Image transormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Grayscale(),\n",
    "        transforms.CenterCrop(240),\n",
    "        transforms.Resize((24, 24)),\n",
    "        #transforms.Normalize((0.5,), (0.5,)) ,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of dataset\n",
    "DATASET_DIR = Path(\"dataset\") \n",
    "# create the directory for well-trained neural network models\n",
    "STATEDICTS_DIR = Path(\"statedicts\")\n",
    "STATEDICTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a visual example of transformed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape torch.Size([3, 24, 24])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAi0UlEQVR4nO3de3DUdZrv8U93p7tzIekQLrlIQBAVb+AsCjLqrI45XGaLEqVq1WPVQYtyzjjBLcxYbrE1irqek6Oz5VhOMbjn1IyOW+WNmRVHdwePEyWMDpcSB0dnZxhg4wBCgqC5knT68jt/uGRPRtD08034dsL7VdVVJN1Pnm9++XV/uknn+4SCIAgEAMBpFva9AADAmYkAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOBFge8F/LlsNqtDhw6ptLRUoVDI93IAADkKgkBdXV2qqalROHzq1zl5F0CHDh1SbW2t72UAABwdOHBAU6ZMOeX1eRdApaWlkqT/8t+/rWgsnnP9vza/6tR/TvV55tpJhU6tpesWmUsLOjrMtZtf/rm5VpIuLK8w146vKHLqXTH+MnPtOapy6l1+1oXm2tlft58sfzhebq6VpI5In7n202P9Tr3/7afrzbUf/fE9c+2EWMxcK0n9EfuOZUWZqFPvPvWaa6OxiFPvkqIyU106nVLTm68PPJ6fSt4F0In/dovG4orGcw8gRdwOeCRqP1kK3M4zqcj+YBxNJs21oQK308DtmLkdtGjc/kBeqGKn3oWF48y1JePsP+ui0Bffqb9M0uFEjfe6BVA0arhP/4dIxH6eFjie41mHACoIufUucHiYLihwezyMOt4/v+zXKCP2JoR169bp7LPPVmFhoebPn68dO3aMVCsAwCg0IgH0wgsvqKGhQWvXrtW7776rOXPmaNGiRTpy5MhItAMAjEIjEkCPPfaY7rjjDt1+++268MIL9eSTT6q4uFg//vGPR6IdAGAUGvYA6u/v186dO1VXV/efTcJh1dXVaevWrZ+7fTKZVGdn56ALAGDsG/YAOnr0qDKZjCorKwd9vrKyUq2trZ+7fWNjoxKJxMCFt2ADwJnB+04Ia9asUUdHx8DlwIEDvpcEADgNhv1t2BMnTlQkElFbW9ugz7e1tamq6vN/dxGPxxW3vN0aADCqDfsroFgsprlz56qpqWngc9lsVk1NTVqwYMFwtwMAjFIj8oeoDQ0NWrFihS677DLNmzdPjz/+uHp6enT77bePRDsAwCg0IgF000036eOPP9b999+v1tZWXXrppdq0adPn3pgAADhzjdhWPKtWrdKqVatG6ssDAEa5vNsL7oRvnHOpiopy36tr98/fduq790/vmmujFdOdegcbfmquHX/+DHNtw3d+ZK6VpA33P2Gufa/9fafe14Z+Z65Nfe2vnXp3yL7/3h/32H/9Ovsr9s1EJan5N/be0UjGqXfXkY/NtfEC+/f9Sci2qeYJ48P2PfCiUfs+cpIUytj3cyuOuW32Oy5q22sxpaGt2fvbsAEAZyYCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAAL/J2HtAP3vqlItFYznV/fV+DU9+X/+8/mWt/+/Zep96Tp1eba+eX1pprPznoNjNk2be+Y67t73SblfJRz2vm2rJDx516K9NiLt3wwgvm2m/9D7c5Rn9ZFjfX3t5wj1Pv6756tbn2D3/oMtf29veaayWpIPeHogHpbMipd2ncXt8ddXuInxQvMdVFhjg/iVdAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4EQqCwG0//GHW2dmpRCKhWSvqFInlvpX41Zd+w6l/X9w+miATcRtrkOxLmmtj3fa+PSXt9mJJoeKp5tqp77mNsNh/aIe59sMWh4Mm6dLKtLn2V4ftW+zHs+PMtZJ01nj7WIOjXVGn3onQx+ba33yaMddOLO4x10pScY/9vt0fTTn1LovZf97F5WVOvSeXV5jqUqmU/vlffq6Ojg6VlZ16DbwCAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOBF7gN3TpMFF35DscLcZ3Aci7vN/ZjqMB7paNs+p96bNr1mrp172D6v5HfJY+ZaSTo/FDHX/uq420yeVOi4ubYo7Xb6/6HFPp+m1KFvNJt1qJa62hLm2s6IW+/SrH0OUmmmz1zb8YnLEZcOhdvNtTUhtzlhnSX2x7R04HaO9/X32vqmhzYri1dAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4EQoCh/kDI6Czs1OJREITzpqocDj3fIwn3b6d3pB9i/1AblvVZzIxc20sZN/mvqDAXitJ/Q7fd0G/2zHrC9tHQUQCt95h2c+1wGGsQTgTN9dKUl9pmbm2LFbo1Pu4w0iFoPsTc21h1u25dlj28yxbYK+VpClx+yiJw0XFTr1nltrq0+m03t75a3V0dKis7NTnG6+AAABeEEAAAC8IIACAF8MeQA888IBCodCgy6xZs4a7DQBglHMbGH4KF110kX75y1/+Z5OCEWkDABjFRiQZCgoKVFVVNRJfGgAwRozI74D27NmjmpoazZgxQ7feeqv2799/ytsmk0l1dnYOugAAxr5hD6D58+fr6aef1qZNm7R+/Xq1tLTo6quvVldX10lv39jYqEQiMXCpra0d7iUBAPLQiP8hant7u6ZNm6bHHntMK1eu/Nz1yWRSyWRy4OPOzk7V1tbyh6g54g9Rc8cfouaOP0TNHX+IeurzbcTfHVBeXq7zzjtPe/fuPen18Xhc8bjbHQoAMPqM+N8BdXd3a9++faqurh7pVgCAUWTYA+iee+5Rc3OzPvzwQ/3617/WDTfcoEgkoltuuWW4WwEARrFh/y+4gwcP6pZbbtGxY8c0adIkXXXVVdq2bZsmTZo03K0AAKPYsAfQ888/P9xfEgAwBuXvFgXprOk/CNuzbt9S1OHdZIlQ2qm3y19AZcL2dYeybu8cLMoWmWuPh5JffqMvUODwRrZw2O37Lgkcvu+s/d2WqWjKXCtJ5xba38k2c4rb/2TsONJjrq2+4i/Mtcf3tZprJWlc0GuuDcfs726VpNb9h8215cFxp949iprqMhraYyGbkQIAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwIm/nAcWyUYUN+VgQdpvJk3WYL9OXdZv7EQ7Z59NksvYZL4VFDt+0pJJMubm297jbc6DiwqPm2nDWbRZRd2Cfy1OQtdeWRyaYayVp+gUXmGsvO8+t965NvzHXht/dYa79n4+/bK6VpMcfvtNc2xm2zdQ5IRvY7yPF9rFTkqS+PttjUiYztDpeAQEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4EXejmPoUlYhS6HDSANJKo7Y65NyG2sQcVh7kelgfea4Q60kKf6JuTQcdjtm6aR9/EZf2j7CQpIUte91XxgeZ64tjcXNtZLUdrTfXLu79F+cetekJ5prd3XYj/cv3t9srv2M/bl6R2ubU+f2oMdePMSxCKcyq3qOqS6VTunfPnjnS2/HKyAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXuTtPKCCdIFC4dzzMR1EnfqGo/b5MtGs2+yNXof6/gL7jzLhuG6li8ylyWyXU2uXcSelYfvPWpL6HZ6/BSH7bJsDkW5zrSTd942LzbUfFH7VqfdH7/zMXDuuzz5D6Xc7XjPXSlI6sJ/jn/Qcc+o9oShiri3qL3XqHTOOzAqlhnY7XgEBALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOBF3o5jiBQkFTaMYyiy73IvSeoJQubacNhtrEFVxL7t+qfZfnNtb5/bWIJQxH7QC7JD3Lf9FMoM58gJvVm30R3RsH3tfbKfZzWdZeZaSXr/4CFz7eHsp069J8+sNdcW1trHGlQ5jt7ojsTtveX28+pKx8y1E75yqVPvopBt9EfBEO8bvAICAHhBAAEAvCCAAABe5BxAW7Zs0dKlS1VTU6NQKKSNGzcOuj4IAt1///2qrq5WUVGR6urqtGfPnuFaLwBgjMg5gHp6ejRnzhytW7fupNc/+uijeuKJJ/Tkk09q+/btKikp0aJFi9TX1+e8WADA2JHzu+CWLFmiJUuWnPS6IAj0+OOP67vf/a6uv/56SdIzzzyjyspKbdy4UTfffLPbagEAY8aw/g6opaVFra2tqqurG/hcIpHQ/PnztXXr1pPWJJNJdXZ2DroAAMa+YQ2g1tZWSVJlZeWgz1dWVg5c9+caGxuVSCQGLrW19r8TAACMHt7fBbdmzRp1dHQMXA4cOOB7SQCA02BYA6iqqkqS1NbWNujzbW1tA9f9uXg8rrKyskEXAMDYN6wBNH36dFVVVampqWngc52dndq+fbsWLFgwnK0AAKNczu+C6+7u1t69ewc+bmlp0a5du1RRUaGpU6dq9erVevjhh3Xuuedq+vTpuu+++1RTU6Nly5YN57oBAKNczgH0zjvv6Nprrx34uKGhQZK0YsUKPf3007r33nvV09Ojb37zm2pvb9dVV12lTZs2qbCwcPhWDQAY9XIOoGuuuUZBcOpdn0OhkB566CE99NBDTgsDAIxt3t8FBwA4M+XtPKDCIKNwkM25rrfAPlNHkuKh3HueEDgezqMZ+8ySkMO3HaSK7MWSSrL2dScz9lknktTuMA+oKOQ2PCoosPce328/V7rlNpPnX3/2C3Ntv8O6JaksljTXtjvMb6oK2+/XktQXtq+7ODTOqXfQW2yunew4B+l4xvbAksoO7XjzCggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC/ydhxD5vKlCqK5b9Vftvklp74fR+yjCaIF/U69M4X2H0f5WfYt32PVpx4wOBTBuHPNteftdRuJ8N/+9u/MtW+/+IhT779c9h1zbWHBx+baC2qvMNdK0r9nPzLXjsuWOPX+ySNPmGvf+d2/m2sf+OH/NtdK0qs/uc1c+0FLhVPv80vtjyvJ3i6n3uMLbKMgUtmhvbbhFRAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF7k7TiGhV+9ULHCwpzrjn1tplPfWL9DJqd7nHpnQ/beifgEc23HeLfTYEpJubn205+/4dS7uN0+WqC7y22r+vJpF5prD//zi+baH2Umm2sl6e++Zh/dEXJ8zjr7gq+aa9/f97659o233jTXStJf/Vf7GImD/2utU+/elG0kgiTFo27jTnpCWVNdaoh1vAICAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4EXezgPqDZJKB6Gc6/oyEae+IYcjElLMqXfUNnpDkpQJ7HM/Sh1Pg6zDuo+l3OaVNP6DfU7LkcKDTr2vixSZa8umnmeundTyU3OtJL2eKTfXlvR86tT793/aYa5NOzxfbu86YK6VpLJx15pr+8Mpp959kfHm2kq5/bwKjXfP8BDreAUEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXeTuOIZLqV0Ek93yMxdy+pZBhBMQJmWjcqXdx2N47Hoqaa3vSbiMssg5PY0qLAqfer2zcYK594q7LnXp37fqVufZP2/7JXHvZt35srpWkKyb2mmuD8W4/r0R4hrn27Q9+Z659b9/PzLWSNLP7b821acdxDFWxPnNtss/tvl0Yss1jCDJDq+MVEADACwIIAOAFAQQA8CLnANqyZYuWLl2qmpoahUIhbdy4cdD1t912m0Kh0KDL4sWLh2u9AIAxIucA6unp0Zw5c7Ru3bpT3mbx4sU6fPjwwOW5555zWiQAYOzJ+S1jS5Ys0ZIlS77wNvF4XFVVVeZFAQDGvhH5HdDmzZs1efJknX/++brzzjt17NixU942mUyqs7Nz0AUAMPYNewAtXrxYzzzzjJqamvTII4+oublZS5YsUeYU7wtvbGxUIpEYuNTW1g73kgAAeWjY/xD15ptvHvj3JZdcotmzZ+ucc87R5s2bdd11133u9mvWrFFDQ8PAx52dnYQQAJwBRvxt2DNmzNDEiRO1d+/ek14fj8dVVlY26AIAGPtGPIAOHjyoY8eOqbq6eqRbAQBGkZz/C667u3vQq5mWlhbt2rVLFRUVqqio0IMPPqjly5erqqpK+/bt07333quZM2dq0aJFw7pwAMDolnMAvfPOO7r22msHPj7x+5sVK1Zo/fr1+u1vf6uf/OQnam9vV01NjRYuXKi///u/VzzutlEnAGBsyTmArrnmGgXBqXfEfe2115wWBAA4M7AXHADAi7ydB5RJZpRWOvfCbIlbX4fZG5lI0ql3f1Bsro2H7M8lyqNu80p6jTNDJCnT4/Zfsw8/cr+5dsq133XqPaHAvtvHzX/zgLk2M8l+jkrS8VSHubY4qHDqXeTwiBMO9Ztrx32UtTeW1Nr5obk23u82kyed6jbX9hba75uSFApsxy2toT2m8AoIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAv8nYcQzKTUTaT+1biQda+1bwkhTNRc21Rxi3Pw1H71umhuH2L/lCk0FwrSdFQyFzb2f2hU++jfdXm2o+O/sqpd+VfXWOuXfOP/8dcW1tYaq6VpPcPtJlrn3zyH5x6V006y1xbUmC/fxVF7PdrSYpFy821Hb32cQqSFIrZv+9Qn9tjUipsi4h0emhjHHgFBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAF3k7jkFB8NklR6m0fTSAJBWEkubajOuW70HEXNsre21pf8xcK0mZqP37nlRV5dT7vV2bzbXnxN3GUPzp04/NtZm+EnPtpz32cQqSdKjjsLk2FO536p3OpMy1fRH78+Xj7W7PtTcefNFcmxg/wal3V9cn5tpw1u37LgnbxrwESg/pdrwCAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOBF3s4D6k+lFIRzz8dwJOvUN9lvnydUEHM7nD0FQ5uhcTKTA3vv9ozb85Bs0j5DKVZY6tQ7OmWhubak902n3uefXWuuzezeb679ytQic60kHe+uNteG++xzpyQpLPvsqGjYft9sk9ucsL6+YnNtT8cxp94uU6syqdxnqv3/+sK2+3Y6yzwgAEAeI4AAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgRd6OY4iWRBWN5751e3fEvm26JIXT9pEIQTjj1LssY9+qvj1l/1HOjLqtuzjTaa49mO5z6j0lZh8F0XnYbaxBMvOJvbY/bq79beskc60kjbvYfq5kC9229w867eNSwumUvXHa7RyfFCsx17Zn3EZBTJb9PtIfcvt5xY13r1BmaD9nXgEBALwggAAAXhBAAAAvcgqgxsZGXX755SotLdXkyZO1bNky7d69e9Bt+vr6VF9frwkTJmjcuHFavny52trahnXRAIDRL6cAam5uVn19vbZt26bXX39dqVRKCxcuVE9Pz8Bt7r77br3yyivasGGDmpubdejQId14443DvnAAwOiW09thNm3aNOjjp59+WpMnT9bOnTv1ta99TR0dHfrRj36kZ599Vl//+tclSU899ZQuuOACbdu2TVdcccXwrRwAMKo5/Q6oo6NDklRRUSFJ2rlzp1KplOrq6gZuM2vWLE2dOlVbt2496ddIJpPq7OwcdAEAjH3mAMpms1q9erWuvPJKXXzxxZKk1tZWxWIxlZeXD7ptZWWlWltbT/p1GhsblUgkBi61tbXWJQEARhFzANXX1+uDDz7Q888/77SANWvWqKOjY+By4MABp68HABgdTH8SvWrVKr366qvasmWLpkyZMvD5qqoq9ff3q729fdCroLa2NlVVVZ30a8XjccXj9r8KBwCMTjm9AgqCQKtWrdJLL72kN954Q9OnTx90/dy5cxWNRtXU1DTwud27d2v//v1asGDB8KwYADAm5PQKqL6+Xs8++6xefvlllZaWDvxeJ5FIqKioSIlEQitXrlRDQ4MqKipUVlamu+66SwsWLOAdcACAQXIKoPXr10uSrrnmmkGff+qpp3TbbbdJkr7//e8rHA5r+fLlSiaTWrRokX74wx8Oy2IBAGNHTgEUBF++s2phYaHWrVundevWmRcFABj72AsOAOBF3s4DKlZYsVDu+ZiU29wPFdrnlcTcxn6oL2T/AjUOc4zaYxFzrSQdb7PPaRlfNs2p9zkz7c+hdqQOOvWujNvPlR199hkvPVGHuTiSDr73G3Nt2HG+TDZ11Fzb12+fl1Va4vZO257OLnNtQdq+bkk6VtBhrnUc9aVIxhYRaeYBAQDyGQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAAL/J2HEOnJMsm5rGIfYt8SQo7HJLMEAb2fWHviH3v9O4hbn9+MuPVa66VpJKudnNt1fmznXq/u8c+UuGs8ec69W7749vm2u6M/bnfBYVF5lpJSpwzz1zbv98+RkKSbn/ib+y9++1jKAovLDPXStKh7uPm2ljWbXzG8dQ4c22Q7XfqXRKzjaEIh4b2WMYrIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABe5O08oFQ2q8Aw4yYUdpvJE3GI5Akht1lEfamYuTYbsvcdn3E7DeIOz2MunPEXTr07Ptpprn332EdOvX+x4R1z7XnjzzbXbj3ynrlWkkqjaXPte2/NdeqdtY/VUSxjX/fEyAX2xpJK221zcSQpk3WbtxXL2u9fkcDtMaktU2yqy2aYBwQAyGMEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4kXe7YQfBZ7tZp5NJ2xfIOGwLLSkbHtourifT77gbdkr2+iBk3wW8L+K2W29g/VlJ6umx7zIsSX0OvVPplFPvzBB3/B3u3um0fVdoSUqF7PXH+xy2s5aUdjhmLsc7neo310pSst9+nrmsW5KCrMMO/4Fb72zI9nh6YjfsE4/npxIKvuwWp9nBgwdVW1vrexkAAEcHDhzQlClTTnl93gVQNpvVoUOHVFpaqtBJ0rezs1O1tbU6cOCAysrKPKxw9OGY5Y5jljuOWe7G6jELgkBdXV2qqalROHzq3/Tk3X/BhcPhL0zME8rKysbUD+x04JjljmOWO45Z7sbiMUskEl96G96EAADwggACAHgx6gIoHo9r7dq1isfjvpcyanDMcscxyx3HLHdn+jHLuzchAADODKPuFRAAYGwggAAAXhBAAAAvCCAAgBejLoDWrVuns88+W4WFhZo/f7527Njhe0l564EHHlAoFBp0mTVrlu9l5ZUtW7Zo6dKlqqmpUSgU0saNGwddHwSB7r//flVXV6uoqEh1dXXas2ePn8XmiS87ZrfddtvnzrvFixf7WWweaGxs1OWXX67S0lJNnjxZy5Yt0+7duwfdpq+vT/X19ZowYYLGjRun5cuXq62tzdOKT59RFUAvvPCCGhoatHbtWr377ruaM2eOFi1apCNHjvheWt666KKLdPjw4YHLW2+95XtJeaWnp0dz5szRunXrTnr9o48+qieeeEJPPvmktm/frpKSEi1atEh9fX2neaX548uOmSQtXrx40Hn33HPPncYV5pfm5mbV19dr27Ztev3115VKpbRw4UL19PQM3Obuu+/WK6+8og0bNqi5uVmHDh3SjTfe6HHVp0kwisybNy+or68f+DiTyQQ1NTVBY2Ojx1Xlr7Vr1wZz5szxvYxRQ1Lw0ksvDXyczWaDqqqq4Hvf+97A59rb24N4PB4899xzHlaYf/78mAVBEKxYsSK4/vrrvaxnNDhy5EggKWhubg6C4LNzKhqNBhs2bBi4ze9///tAUrB161ZfyzwtRs0roP7+fu3cuVN1dXUDnwuHw6qrq9PWrVs9riy/7dmzRzU1NZoxY4ZuvfVW7d+/3/eSRo2Wlha1trYOOucSiYTmz5/POfclNm/erMmTJ+v888/XnXfeqWPHjvleUt7o6OiQJFVUVEiSdu7cqVQqNeg8mzVrlqZOnTrmz7NRE0BHjx5VJpNRZWXloM9XVlaqtbXV06ry2/z58/X0009r06ZNWr9+vVpaWnT11Verq8ttBs+Z4sR5xTmXm8WLF+uZZ55RU1OTHnnkETU3N2vJkiXOc3HGgmw2q9WrV+vKK6/UxRdfLOmz8ywWi6m8vHzQbc+E8yzvdsPG8FmyZMnAv2fPnq358+dr2rRpevHFF7Vy5UqPK8NYdvPNNw/8+5JLLtHs2bN1zjnnaPPmzbruuus8rsy/+vp6ffDBB/wu9j+MmldAEydOVCQS+dw7Q9ra2lRVVeVpVaNLeXm5zjvvPO3du9f3UkaFE+cV55ybGTNmaOLEiWf8ebdq1Sq9+uqrevPNNweNnKmqqlJ/f7/a29sH3f5MOM9GTQDFYjHNnTtXTU1NA5/LZrNqamrSggULPK5s9Oju7ta+fftUXV3teymjwvTp01VVVTXonOvs7NT27ds553Jw8OBBHTt27Iw974Ig0KpVq/TSSy/pjTfe0PTp0wddP3fuXEWj0UHn2e7du7V///4xf56Nqv+Ca2ho0IoVK3TZZZdp3rx5evzxx9XT06Pbb7/d99Ly0j333KOlS5dq2rRpOnTokNauXatIJKJbbrnF99LyRnd396Bn5i0tLdq1a5cqKio0depUrV69Wg8//LDOPfdcTZ8+Xffdd59qamq0bNkyf4v27IuOWUVFhR588EEtX75cVVVV2rdvn+69917NnDlTixYt8rhqf+rr6/Xss8/q5ZdfVmlp6cDvdRKJhIqKipRIJLRy5Uo1NDSooqJCZWVluuuuu7RgwQJdccUVnlc/wny/DS9XP/jBD4KpU6cGsVgsmDdvXrBt2zbfS8pbN910U1BdXR3EYrHgrLPOCm666aZg7969vpeVV958881A0ucuK1asCILgs7di33fffUFlZWUQj8eD6667Lti9e7ffRXv2Rcfs+PHjwcKFC4NJkyYF0Wg0mDZtWnDHHXcEra2tvpftzcmOlaTgqaeeGrhNb29v8O1vfzsYP358UFxcHNxwww3B4cOH/S36NGEcAwDAi1HzOyAAwNhCAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC/+HzE16jJqP77lAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def visualize_dataset_sample(dataset_path: str, sample_idx: int = 2) -> None:\n",
    "    \"\"\"\n",
    "    Displays a visual representation of an example image from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the directory where the dataset is stored.\n",
    "        sample_idx: The index of the sample to be visualized. Defaults to 1200.\n",
    "\n",
    "    Returns:\n",
    "        None: This function only displays an image and does not return any value.\n",
    "    \"\"\"\n",
    "    dataset = CNNDataset(dataset_path, transform=IMAGE_TRANSFORM)\n",
    "    img_example,label_data_example = dataset[sample_idx]\n",
    "    print('Image shape',np.shape(img_example))\n",
    "    img_example = img_example.permute(1, 2, 0)\n",
    "    plt.imshow(img_example, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(label_data_example)\n",
    "   \n",
    "\n",
    "visualize_dataset_sample(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1986 samples\n"
     ]
    }
   ],
   "source": [
    "def load_dataloaders(\n",
    "    dataset_path: PathLike,\n",
    "    val_ratio: float = 0.2,\n",
    "    test_ratio: float = 0.2,\n",
    "    batch_size: int = 32,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads and returns data loaders for training, validation, and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the directory where the dataset is stored.\n",
    "        val_ratio: The ratio of the dataset to be used for validation.\n",
    "                    It must be between 0 and 1. Defaults to 0.2.\n",
    "        test_ratio: The ratio of the dataset to be used for testing.\n",
    "                    It must be between 0 and 1. Defaults to 0.3.\n",
    "        batch_size: The number of samples per batch to load. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        train_dataloader: The DataLoader instances for the training.\n",
    "        val_dataloader: The DataLoader instances for the validation.\n",
    "        test_dataloader: The DataLoader instances for the testing.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the sum of val_ratio and test_ratio is larger than 1.0\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        0.0 <= val_ratio <= 1.0\n",
    "    ), \"Validation ratio needs to be in the interval [0, 1].\"\n",
    "    assert 0.0 <= test_ratio <= 1.0, \"Test ratio needs to be in the interval [0, 1].\"\n",
    "    assert (\n",
    "        val_ratio + test_ratio\n",
    "    ) <= 1.0, \"The sum of val and test ratio needs to be in the interval [0, 1].\"\n",
    "\n",
    "\n",
    "    dataset = CNNDataset(dataset_path, transform=IMAGE_TRANSFORM)\n",
    "    print(f\"The dataset contains {len(dataset)} samples\")\n",
    "\n",
    "    train_size = int(len(dataset) * (1.0 - val_ratio - test_ratio)) +1\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    test_size = int(len(dataset) * test_ratio)\n",
    "    # print(train_size,val_size,test_size)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = load_dataloaders(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module, eval_loader: DataLoader, \n",
    ") -> Tuple[float]:\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a theta model on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to be evaluated.\n",
    "        eval_loader: The DataLoader for the evaluation dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "\n",
    "    count = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (image, label) in enumerate(eval_loader):\n",
    "            # Calculate loss here by calling the loss function\n",
    "            pred = model(image).squeeze()\n",
    "            batch_loss = loss_fn(pred,label.float())\n",
    "            running_loss += batch_loss.item()\n",
    "            count += 1\n",
    "\n",
    "    loss = running_loss / count\n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters:  8647\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TASK 1b.1.1: CREATE THETA MODEL HERE\"\"\"\n",
    "\n",
    "class CNNObstacleDetect(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3,32,3)    # 32x22x22\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.avgPool1 = torch.nn.AvgPool2d(2)   # 32x11x11\n",
    "        self.conv2 = torch.nn.Conv2d(32,10,3)     # 10x9x9\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.avgPool2 = torch.nn.AvgPool2d(2)   # 10x4x4\n",
    "        self.flat1 = torch.nn.Flatten()         # 160\n",
    "        self.fc1 = torch.nn.Linear(160,30)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(30,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.avgPool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgPool2(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\"\"\"TASK 1b.1.1: END\"\"\"\n",
    "\n",
    "model_CNN = CNNObstacleDetect()\n",
    "total_params_theta = sum(p.numel() for p in model_CNN.parameters())\n",
    "print(\"Total number of trainable model parameters: \", total_params_theta)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of runs (i.e., different random seeds)\n",
    "num_runs = 1  # Change to 1 until you get it to work once (need to be 3)\n",
    "# number of epochs we train each model for\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61292876405e45c781de2aba69e0277b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 finished with a test loss of 0.4575\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Starting run {run}\")\n",
    "    # set new random seed\n",
    "    manual_seed(seed=run)\n",
    "    # this code reinitializes the parameters of the model on each loop\n",
    "    model_CNN = CNNObstacleDetect()\n",
    "\n",
    "    # define the optimizer\n",
    "    optimizer = torch.optim.SGD(model_CNN.parameters(), lr=0.5e-2)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for image,label in train_loader:\n",
    "            preds = model_CNN(image)\n",
    "\n",
    "            batch_size = preds.shape[0]\n",
    "            label_len = preds.shape[1]\n",
    "            label = label.view(batch_size,label_len)\n",
    "        \n",
    "\n",
    "            loss = loss_fn(preds,label.float())\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "    run_test_loss = evaluate_model(\n",
    "        model_CNN,test_loader,\n",
    "    )\n",
    "    print(\n",
    "        f\"Run {run} finished with a test loss of {run_test_loss:.4}\"\n",
    "    )\n",
    "\n",
    "    # save the model\n",
    "    torch.save(\n",
    "        model_CNN.state_dict(),\n",
    "        STATEDICTS_DIR / f\"obstacle_danger_model-{run}.pth\",\n",
    "        _use_new_zipfile_serialization=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_runs):\n\u001b[1;32m      4\u001b[0m     model_CNN\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m      5\u001b[0m         torch\u001b[38;5;241m.\u001b[39mload(STATEDICTS_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobstacle_danger_model-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     )\n\u001b[0;32m----> 7\u001b[0m     run_test_loss, run_test_error \u001b[38;5;241m=\u001b[39m evaluate_model(\n\u001b[1;32m      8\u001b[0m         model_CNN, test_loader,\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     test_error_across_runs[run] \u001b[38;5;241m=\u001b[39m run_test_error\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction error of theta model across runs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(test_error_across_runs)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m += \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(test_error_across_runs)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rad.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "# evaluate the theta model on the test set\n",
    "test_error_across_runs_theta = np.zeros((num_runs,))\n",
    "for run in range(num_runs):\n",
    "    model_CNN.load_state_dict(\n",
    "        torch.load(STATEDICTS_DIR / f\"obstacle_danger_model-{run}.pth\")\n",
    "    )\n",
    "    run_test_loss, run_test_error = evaluate_model(\n",
    "        model_CNN, test_loader,\n",
    "    )\n",
    "    test_error_across_runs[run] = run_test_error\n",
    "print(\n",
    "    f\"Prediction error of theta model across runs: {np.mean(test_error_across_runs):.4} += {np.std(test_error_across_runs):.4} rad.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
